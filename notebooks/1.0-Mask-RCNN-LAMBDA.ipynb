{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1NZ-_x8E0zRW",
    "papermill": {
     "duration": 0.023251,
     "end_time": "2021-10-30T15:50:05.006187",
     "exception": false,
     "start_time": "2021-10-30T15:50:04.982936",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "id": "oQYSI0Y00zRX",
    "papermill": {
     "duration": 2.167855,
     "end_time": "2021-10-30T15:50:07.197808",
     "exception": false,
     "start_time": "2021-10-30T15:50:05.029953",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The notebooks is self-contained\n",
    "# It has very few imports\n",
    "# No external dependencies (only the model weights)\n",
    "# No train - inference notebooks\n",
    "# We only rely on Pytorch\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.transforms import ToPILImage\n",
    "from torchvision.transforms import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "from torchvision.models.detection.anchor_utils import AnchorGenerator\n",
    "from torchvision.models.detection.rpn import RPNHead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Y7fwE02H0zRY",
    "papermill": {
     "duration": 0.032827,
     "end_time": "2021-10-30T15:50:07.253793",
     "exception": false,
     "start_time": "2021-10-30T15:50:07.220966",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fix randomness\n",
    "\n",
    "def fix_all_seeds(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "fix_all_seeds(2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NqZ4eNVK0zRZ",
    "papermill": {
     "duration": 0.022674,
     "end_time": "2021-10-30T15:50:07.29946",
     "exception": false,
     "start_time": "2021-10-30T15:50:07.276786",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device in use : cuda\n"
     ]
    }
   ],
   "source": [
    "HOME = os.path.join('/home/ubuntu/kaggle-sartorius/')\n",
    "RAW_DATA = os.path.join(HOME,'data/raw')\n",
    "INTERIM_DATA = os.path.join(HOME,'data/interim')\n",
    "PROCESSED_DATA = os.path.join(HOME,'data/processed')\n",
    "\n",
    "TRAIN_CSV  = os.path.join(RAW_DATA,'train.csv')  \n",
    "TRAIN_PATH = os.path.join(RAW_DATA,'train')  \n",
    "TEST_PATH  = os.path.join(RAW_DATA,'test')  \n",
    "PRETRAINED_WEIGHTS_PATH = os.path.join(RAW_DATA,'cocopre','maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth')\n",
    "\n",
    "WIDTH  = 704\n",
    "HEIGHT = 520\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "# Reduced the train dataset to 5000 rows\n",
    "TEST = False\n",
    "\n",
    "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "print(f\"Device in use : {DEVICE}\")\n",
    "\n",
    "RESNET_MEAN = (0.485, 0.456, 0.406)\n",
    "RESNET_STD  = (0.229, 0.224, 0.225)\n",
    "\n",
    "\n",
    "\n",
    "# No changes tried with the optimizer yet.\n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 0.0005\n",
    "\n",
    "# Changes the confidence required for a pixel to be kept for a mask. \n",
    "# Only used 0.5 till now.\n",
    "MASK_THRESHOLD = 0.5\n",
    "\n",
    "# Normalize to resnet mean and std if True.\n",
    "NORMALIZE = False \n",
    "\n",
    "\n",
    "# Use a StepLR scheduler if True. Not tried yet.\n",
    "USE_SCHEDULER = False\n",
    "\n",
    "# Number of epochs\n",
    "NUM_EPOCHS = 100\n",
    "\n",
    "\n",
    "BOX_DETECTIONS_PER_IMG = 539\n",
    "\n",
    "\n",
    "MIN_SCORE = 0.59\n",
    "\n",
    "\n",
    "resize_factor = False\n",
    "\n",
    "\n",
    "\n",
    "# cell type specific thresholds\n",
    "cell_type_dict = {\"astro\": 1, \"cort\": 2, \"shsy5y\": 3}\n",
    "mask_threshold_dict = {1: 0.55, 2: 0.75, 3:  0.6}\n",
    "min_score_dict = {1: 0.55, 2: 0.75, 3: 0.5}\n",
    "\n",
    "\n",
    "PCT_IMAGES_VALIDATION = 0.075\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "VSPe6quz0zRZ",
    "lines_to_next_cell": 1,
    "outputId": "e2cca0e2-0ada-471b-e688-33ee16049407",
    "papermill": {
     "duration": 0.071974,
     "end_time": "2021-10-30T15:50:07.394165",
     "exception": false,
     "start_time": "2021-10-30T15:50:07.322191",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Reduced the train dataset to 5000 rows\n",
    "# TEST = False\n",
    "\n",
    "# if os.path.exists(\"../input/sartorius-cell-instance-segmentation\"):\n",
    "#     # running on kaggle\n",
    "#     data_directory = '../input/sartorius-cell-instance-segmentation'\n",
    "#     DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "#     BATCH_SIZE = 8\n",
    "#     NUM_EPOCHS = 4\n",
    "\n",
    "# elif 'google.colab' in str(get_ipython()):\n",
    "#     # running on CoLab\n",
    "#     from google.colab import drive\n",
    "#     drive.mount('/content/drive')\n",
    "#     data_directory = '/content/drive/MyDrive/input'\n",
    "#     DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "#     BATCH_SIZE = 1\n",
    "#     NUM_EPOCHS = 5\n",
    "    \n",
    "# else:\n",
    "#     data_directory = 'input'\n",
    "#     DEVICE = torch.device('cpu')\n",
    "#     BATCH_SIZE = 2\n",
    "#     NUM_EPOCHS = 1\n",
    "#     TEST = True\n",
    "\n",
    "# TRAIN_CSV = f\"{data_directory}/train.csv\"\n",
    "# TRAIN_PATH = f\"{data_directory}/train\"\n",
    "# TEST_PATH = f\"{data_directory}/test\"\n",
    "\n",
    "# WIDTH = 704\n",
    "# HEIGHT = 520\n",
    "\n",
    "# resize_factor = False # 0.5\n",
    "\n",
    "# # Normalize to resnet mean and std if True.\n",
    "# NORMALIZE = False\n",
    "# RESNET_MEAN = (0.485, 0.456, 0.406)\n",
    "# RESNET_STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "# # No changes tried with the optimizer yet.\n",
    "# MOMENTUM = 0.9\n",
    "# LEARNING_RATE = 0.001\n",
    "# WEIGHT_DECAY = 0.0005\n",
    "\n",
    "# # Changes the confidence required for a pixel to be kept for a mask. \n",
    "# # Only used 0.5 till now.\n",
    "# # MASK_THRESHOLD = 0.5\n",
    "# # MIN_SCORE = 0.5\n",
    "# # cell type specific thresholds\n",
    "# cell_type_dict = {\"astro\": 1, \"cort\": 2, \"shsy5y\": 3}\n",
    "# mask_threshold_dict = {1: 0.55, 2: 0.75, 3:  0.6}\n",
    "# min_score_dict = {1: 0.55, 2: 0.75, 3: 0.5}\n",
    "\n",
    "# # Use a StepLR scheduler if True. \n",
    "# USE_SCHEDULER = False\n",
    "\n",
    "# PCT_IMAGES_VALIDATION = 0.075\n",
    "\n",
    "# BOX_DETECTIONS_PER_IMG = 540"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iIpvad7y0zRb",
    "papermill": {
     "duration": 0.022537,
     "end_time": "2021-10-30T15:50:07.439624",
     "exception": false,
     "start_time": "2021-10-30T15:50:07.417087",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "papermill": {
     "duration": 0.034074,
     "end_time": "2021-10-27T04:04:40.693264",
     "exception": false,
     "start_time": "2021-10-27T04:04:40.65919",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ref: https://www.kaggle.com/inversion/run-length-decoding-quick-start\n",
    "def rle_decode(mask_rle, shape, color=1):\n",
    "    '''\n",
    "    mask_rle: run-length as string formated (start length)\n",
    "    shape: (height, width, channels) of array to return\n",
    "    color: color for the mask\n",
    "    Returns numpy array (mask)\n",
    "\n",
    "    '''\n",
    "    s = mask_rle.split()\n",
    "\n",
    "    starts = list(map(lambda x: int(x) - 1, s[0::2]))\n",
    "    lengths = list(map(int, s[1::2]))\n",
    "    ends = [x + y for x, y in zip(starts, lengths)]\n",
    "    if len(shape)==3:\n",
    "        img = np.zeros((shape[0] * shape[1], shape[2]), dtype=np.float32)\n",
    "    else:\n",
    "        img = np.zeros(shape[0] * shape[1], dtype=np.float32)\n",
    "    for start, end in zip(starts, ends):\n",
    "        img[start : end] = color\n",
    "\n",
    "    return img.reshape(shape)\n",
    "\n",
    "\n",
    "def rle_encoding(x):\n",
    "    dots = np.where(x.flatten() == 1)[0]\n",
    "    run_lengths = []\n",
    "    prev = -2\n",
    "    for b in dots:\n",
    "        if (b>prev+1): run_lengths.extend((b + 1, 0))\n",
    "        run_lengths[-1] += 1\n",
    "        prev = b\n",
    "    return ' '.join(map(str, run_lengths))\n",
    "\n",
    "\n",
    "def remove_overlapping_pixels(mask, other_masks):\n",
    "    for other_mask in other_masks:\n",
    "        if np.sum(np.logical_and(mask, other_mask)) > 0:\n",
    "            mask[np.logical_and(mask, other_mask)] = 0\n",
    "    return mask\n",
    "\n",
    "def combine_masks(masks, mask_threshold):\n",
    "    \"\"\"\n",
    "    combine masks into one image\n",
    "    \"\"\"\n",
    "    maskimg = np.zeros((HEIGHT, WIDTH))\n",
    "    # print(len(masks.shape), masks.shape)\n",
    "    for m, mask in enumerate(masks,1):\n",
    "        maskimg[mask>mask_threshold] = m\n",
    "    return maskimg\n",
    "\n",
    "\n",
    "def get_filtered_masks(pred):\n",
    "    \"\"\"\n",
    "    filter masks using MIN_SCORE for mask and MAX_THRESHOLD for pixels\n",
    "    \"\"\"\n",
    "    use_masks = []   \n",
    "    for i, mask in enumerate(pred[\"masks\"]):\n",
    "\n",
    "        # Filter-out low-scoring results. Not tried yet.\n",
    "        scr = pred[\"scores\"][i].cpu().item()\n",
    "        label = pred[\"labels\"][i].cpu().item()\n",
    "        if scr > min_score_dict[label]:\n",
    "            mask = mask.cpu().numpy().squeeze()\n",
    "            # Keep only highly likely pixels\n",
    "            binary_mask = mask > mask_threshold_dict[label]\n",
    "            binary_mask = remove_overlapping_pixels(binary_mask, use_masks)\n",
    "            use_masks.append(binary_mask)\n",
    "\n",
    "    return use_masks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.022763,
     "end_time": "2021-10-30T15:50:07.545798",
     "exception": false,
     "start_time": "2021-10-30T15:50:07.523035",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Metric: mean of the precision values at each IoU threshold\n",
    "\n",
    "Ref: https://www.kaggle.com/theoviel/competition-metric-map-iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "papermill": {
     "duration": 0.042481,
     "end_time": "2021-10-30T15:50:07.612219",
     "exception": false,
     "start_time": "2021-10-30T15:50:07.569738",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_iou(labels, y_pred, verbose=0):\n",
    "    \"\"\"\n",
    "    Computes the IoU for instance labels and predictions.\n",
    "\n",
    "    Args:\n",
    "        labels (np array): Labels.\n",
    "        y_pred (np array): predictions\n",
    "\n",
    "    Returns:\n",
    "        np array: IoU matrix, of size true_objects x pred_objects.\n",
    "    \"\"\"\n",
    "\n",
    "    true_objects = len(np.unique(labels))\n",
    "    pred_objects = len(np.unique(y_pred))\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Number of true objects: {}\".format(true_objects))\n",
    "        print(\"Number of predicted objects: {}\".format(pred_objects))\n",
    "\n",
    "    # Compute intersection between all objects\n",
    "    intersection = np.histogram2d(\n",
    "        labels.flatten(), y_pred.flatten(), bins=(true_objects, pred_objects)\n",
    "    )[0]\n",
    "\n",
    "    # Compute areas (needed for finding the union between all objects)\n",
    "    area_true = np.histogram(labels, bins=true_objects)[0]\n",
    "    area_pred = np.histogram(y_pred, bins=pred_objects)[0]\n",
    "    area_true = np.expand_dims(area_true, -1)\n",
    "    area_pred = np.expand_dims(area_pred, 0)\n",
    "\n",
    "    # Compute union\n",
    "    union = area_true + area_pred - intersection\n",
    "    intersection = intersection[1:, 1:] # exclude background\n",
    "    union = union[1:, 1:]\n",
    "    union[union == 0] = 1e-9\n",
    "    iou = intersection / union\n",
    "    \n",
    "    return iou  \n",
    "\n",
    "def precision_at(threshold, iou):\n",
    "    \"\"\"\n",
    "    Computes the precision at a given threshold.\n",
    "\n",
    "    Args:\n",
    "        threshold (float): Threshold.\n",
    "        iou (np array): IoU matrix.\n",
    "\n",
    "    Returns:\n",
    "        int: Number of true positives,\n",
    "        int: Number of false positives,\n",
    "        int: Number of false negatives.\n",
    "    \"\"\"\n",
    "    matches = iou > threshold\n",
    "    true_positives = np.sum(matches, axis=1) == 1  # Correct objects\n",
    "    false_positives = np.sum(matches, axis=0) == 0  # Missed objects\n",
    "    false_negatives = np.sum(matches, axis=1) == 0  # Extra objects\n",
    "    tp, fp, fn = (\n",
    "        np.sum(true_positives),\n",
    "        np.sum(false_positives),\n",
    "        np.sum(false_negatives),\n",
    "    )\n",
    "    return tp, fp, fn\n",
    "\n",
    "def iou_map(truths, preds, verbose=0):\n",
    "    \"\"\"\n",
    "    Computes the metric for the competition.\n",
    "    Masks contain the segmented pixels where each object has one value associated,\n",
    "    and 0 is the background.\n",
    "\n",
    "    Args:\n",
    "        truths (list of masks): Ground truths.\n",
    "        preds (list of masks): Predictions.\n",
    "        verbose (int, optional): Whether to print infos. Defaults to 0.\n",
    "\n",
    "    Returns:\n",
    "        float: mAP.\n",
    "    \"\"\"\n",
    "    ious = [compute_iou(truth, pred, verbose) for truth, pred in zip(truths, preds)]\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Thresh\\tTP\\tFP\\tFN\\tPrec.\")\n",
    "\n",
    "    prec = []\n",
    "    for t in np.arange(0.5, 1.0, 0.05):\n",
    "        tps, fps, fns = 0, 0, 0\n",
    "        for iou in ious:\n",
    "            tp, fp, fn = precision_at(t, iou)\n",
    "            tps += tp\n",
    "            fps += fp\n",
    "            fns += fn\n",
    "\n",
    "        p = tps / (tps + fps + fns)\n",
    "        prec.append(p)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"{:1.3f}\\t{}\\t{}\\t{}\\t{:1.3f}\".format(t, tps, fps, fns, p))\n",
    "\n",
    "    if verbose:\n",
    "        print(\"AP\\t-\\t-\\t-\\t{:1.3f}\".format(np.mean(prec)))\n",
    "\n",
    "    return np.mean(prec)\n",
    "\n",
    "\n",
    "def get_score(ds, mdl):\n",
    "    \"\"\"\n",
    "    Get average IOU mAP score for a dataset\n",
    "    \"\"\"\n",
    "    mdl.eval()\n",
    "    iouscore = 0\n",
    "    for i in tqdm(range(len(ds))):\n",
    "        img, targets = ds[i]\n",
    "        with torch.no_grad():\n",
    "            result = mdl([img.to(DEVICE)])[0]\n",
    "            \n",
    "        masks = combine_masks(targets['masks'], 0.5)\n",
    "        labels = pd.Series(result['labels'].cpu().numpy()).value_counts()\n",
    "\n",
    "        mask_threshold = mask_threshold_dict[labels.sort_values().index[-1]]\n",
    "        pred_masks = combine_masks(get_filtered_masks(result), mask_threshold)\n",
    "        iouscore += iou_map([masks],[pred_masks])\n",
    "    return iouscore / len(ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.023883,
     "end_time": "2021-10-30T15:50:07.659293",
     "exception": false,
     "start_time": "2021-10-30T15:50:07.63541",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Transformations\n",
    "Just Horizontal and Vertical Flip for now.\n",
    "\n",
    "Normalization to Resnet's mean and std can be performed using the parameter `NORMALIZE` in the top cell.\n",
    "\n",
    "The first 3 transformations come from [this](https://www.kaggle.com/abhishek/maskrcnn-utils) utils package by Abishek, `VerticalFlip` is my adaption of HorizontalFlip, and `Normalize` is of my own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "papermill": {
     "duration": 0.038125,
     "end_time": "2021-10-30T15:50:07.72041",
     "exception": false,
     "start_time": "2021-10-30T15:50:07.682285",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# These are slight redefinitions of torch.transformation classes\n",
    "# The difference is that they handle the target and the mask\n",
    "# Copied from Abishek, added new ones\n",
    "from torchvision import transforms\n",
    "class Compose:\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        for t in self.transforms:\n",
    "            image, target = t(image, target)\n",
    "        return image, target\n",
    "\n",
    "class VerticalFlip:\n",
    "    def __init__(self, prob):\n",
    "        self.prob = prob\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        if random.random() < self.prob:\n",
    "            height, width = image.shape[-2:]\n",
    "            image = image.flip(-2)\n",
    "            bbox = target[\"boxes\"]\n",
    "            bbox[:, [1, 3]] = height - bbox[:, [3, 1]]\n",
    "            target[\"boxes\"] = bbox\n",
    "            target[\"masks\"] = target[\"masks\"].flip(-2)\n",
    "        return image, target\n",
    "\n",
    "class HorizontalFlip:\n",
    "    def __init__(self, prob):\n",
    "        self.prob = prob\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        if random.random() < self.prob:\n",
    "            height, width = image.shape[-2:]\n",
    "            image = image.flip(-1)\n",
    "            bbox = target[\"boxes\"]\n",
    "            bbox[:, [0, 2]] = width - bbox[:, [2, 0]]\n",
    "            target[\"boxes\"] = bbox\n",
    "            target[\"masks\"] = target[\"masks\"].flip(-1)\n",
    "        return image, target\n",
    "\n",
    "class Normalize:\n",
    "    def __call__(self, image, target):\n",
    "        image = F.normalize(image, RESNET_MEAN, RESNET_STD)\n",
    "        return image, target\n",
    "\n",
    "class GaussianBlur:\n",
    "    def __init__(self, kernel_size):\n",
    "        self.kernel_size = kernel_size\n",
    "        \n",
    "    def __call__(self, image, target):\n",
    "        t = transforms.GaussianBlur(self.kernel_size)\n",
    "        image = t(image)\n",
    "        return image, target\n",
    "    \n",
    "    \n",
    "class ToTensor:\n",
    "    def __call__(self, image, target):\n",
    "        image = F.to_tensor(image)\n",
    "        return image, target\n",
    "    \n",
    "\n",
    "def get_transform(train):\n",
    "    transforms = [ToTensor()]\n",
    "    if NORMALIZE:\n",
    "        transforms.append(Normalize())\n",
    "    \n",
    "    # Data augmentation for train\n",
    "    if train: \n",
    "        transforms.append(HorizontalFlip(0.5))\n",
    "        transforms.append(VerticalFlip(0.5))\n",
    "\n",
    "    # if random.random()>0.5:\n",
    "    #     transforms.append(GaussianBlur(3))\n",
    "        \n",
    "    return Compose(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hHT_aovU0zRd",
    "papermill": {
     "duration": 0.022607,
     "end_time": "2021-10-30T15:50:07.76565",
     "exception": false,
     "start_time": "2021-10-30T15:50:07.743043",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Training Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "C9Y03YgA0zRd",
    "papermill": {
     "duration": 0.044667,
     "end_time": "2021-10-30T15:50:07.833528",
     "exception": false,
     "start_time": "2021-10-30T15:50:07.788861",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cell_type_dict = {\"astro\": 1, \"cort\": 2, \"shsy5y\": 3}\n",
    "\n",
    "class CellDataset(Dataset):\n",
    "    def __init__(self, image_dir, df, transforms=None, resize=False):\n",
    "        self.transforms = transforms\n",
    "        self.image_dir = image_dir\n",
    "        self.df = df\n",
    "        \n",
    "        self.should_resize = resize is not False\n",
    "        if self.should_resize:\n",
    "            self.height = int(HEIGHT * resize)\n",
    "            self.width = int(WIDTH * resize)\n",
    "            print(\"image size used:\", self.height, self.width)\n",
    "        else:\n",
    "            self.height = HEIGHT\n",
    "            self.width = WIDTH\n",
    "        \n",
    "        self.image_info = collections.defaultdict(dict)\n",
    "        temp_df = self.df.groupby([\"id\", \"cell_type\"])['annotation'].agg(lambda x: list(x)).reset_index()\n",
    "        for index, row in temp_df.iterrows():\n",
    "            self.image_info[index] = {\n",
    "                    'image_id': row['id'],\n",
    "                    'image_path': os.path.join(self.image_dir, row['id'] + '.png'),\n",
    "                    'annotations': list(row[\"annotation\"]),\n",
    "                    'cell_type': cell_type_dict[row[\"cell_type\"]]\n",
    "                    }\n",
    "            \n",
    "    def get_box(self, a_mask):\n",
    "        ''' Get the bounding box of a given mask '''\n",
    "        pos = np.where(a_mask)\n",
    "        xmin = np.min(pos[1])\n",
    "        xmax = np.max(pos[1])\n",
    "        ymin = np.min(pos[0])\n",
    "        ymax = np.max(pos[0])\n",
    "        return [xmin, ymin, xmax, ymax]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ''' Get the image and the target'''\n",
    "        \n",
    "        img_path = self.image_info[idx][\"image_path\"]\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "        \n",
    "        if self.should_resize:\n",
    "            img = cv2.resize(img, (self.width, self.height))\n",
    "\n",
    "        info = self.image_info[idx]\n",
    "\n",
    "        n_objects = len(info['annotations'])\n",
    "        masks = np.zeros((len(info['annotations']), self.height, self.width), dtype=np.uint8)\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for i, annotation in enumerate(info['annotations']):\n",
    "            a_mask = rle_decode(annotation, (HEIGHT, WIDTH))\n",
    "            \n",
    "            if self.should_resize:\n",
    "                a_mask = cv2.resize(a_mask, (self.width, self.height))\n",
    "            \n",
    "            a_mask = np.array(a_mask) > 0\n",
    "            masks[i, :, :] = a_mask\n",
    "            \n",
    "            boxes.append(self.get_box(a_mask))\n",
    "\n",
    "        # labels\n",
    "        labels = [int(info[\"cell_type\"]) for _ in range(n_objects)]\n",
    "        #labels = [1 for _ in range(n_objects)]\n",
    "        \n",
    "        \n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "\n",
    "        image_id = torch.tensor([idx])\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        iscrowd = torch.zeros((n_objects,), dtype=torch.int64)\n",
    "\n",
    "        # This is the required target for the Mask R-CNN\n",
    "        target = {\n",
    "            'boxes': boxes,\n",
    "            'labels': labels,\n",
    "            'masks': masks,\n",
    "            'image_id': image_id,\n",
    "            'area': area,\n",
    "            'iscrowd': iscrowd\n",
    "        }\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "tmCw3DTL0zRe",
    "papermill": {
     "duration": 0.594812,
     "end_time": "2021-10-30T15:50:08.451409",
     "exception": false,
     "start_time": "2021-10-30T15:50:07.856597",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(73585, 9)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_base = pd.read_csv(TRAIN_CSV, nrows=5000 if TEST else None)\n",
    "# df_base = df_base.sample(5000).reset_index(drop=True)\n",
    "\n",
    "df_base.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "what_to_do_with_duplicates = 'raise'#'drop'#or 'raise'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "pQXGwdnL0zRe",
    "outputId": "08e892c7-aa0c-4b96-de86-7eb8a4234f16",
    "papermill": {
     "duration": 0.151173,
     "end_time": "2021-10-30T15:50:08.629709",
     "exception": false,
     "start_time": "2021-10-30T15:50:08.478536",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cell_type</th>\n",
       "      <th>annotation</th>\n",
       "      <th>quantiles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>a7b1db2a42fc</td>\n",
       "      <td>astro</td>\n",
       "      <td>594</td>\n",
       "      <td>(105.0, 594.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>903d94c69354</td>\n",
       "      <td>astro</td>\n",
       "      <td>351</td>\n",
       "      <td>(105.0, 594.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>2c2cb870da85</td>\n",
       "      <td>astro</td>\n",
       "      <td>174</td>\n",
       "      <td>(105.0, 594.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>1ea4e44e5497</td>\n",
       "      <td>astro</td>\n",
       "      <td>164</td>\n",
       "      <td>(105.0, 594.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>a75cdb426a8e</td>\n",
       "      <td>astro</td>\n",
       "      <td>163</td>\n",
       "      <td>(105.0, 594.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               id cell_type  annotation       quantiles\n",
       "5    a7b1db2a42fc     astro         594  (105.0, 594.0]\n",
       "71   903d94c69354     astro         351  (105.0, 594.0]\n",
       "135  2c2cb870da85     astro         174  (105.0, 594.0]\n",
       "138  1ea4e44e5497     astro         164  (105.0, 594.0]\n",
       "139  a75cdb426a8e     astro         163  (105.0, 594.0]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cell_type</th>\n",
       "      <th>annotation</th>\n",
       "      <th>quantiles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>4425efbbacfc</td>\n",
       "      <td>cort</td>\n",
       "      <td>108</td>\n",
       "      <td>(43.0, 108.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>4b8dc9c901a6</td>\n",
       "      <td>cort</td>\n",
       "      <td>94</td>\n",
       "      <td>(43.0, 108.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>adfd16bee70c</td>\n",
       "      <td>cort</td>\n",
       "      <td>94</td>\n",
       "      <td>(43.0, 108.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>76ad9ac01e2d</td>\n",
       "      <td>cort</td>\n",
       "      <td>94</td>\n",
       "      <td>(43.0, 108.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>44a154410273</td>\n",
       "      <td>cort</td>\n",
       "      <td>89</td>\n",
       "      <td>(43.0, 108.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               id cell_type  annotation      quantiles\n",
       "165  4425efbbacfc      cort         108  (43.0, 108.0]\n",
       "185  4b8dc9c901a6      cort          94  (43.0, 108.0]\n",
       "186  adfd16bee70c      cort          94  (43.0, 108.0]\n",
       "188  76ad9ac01e2d      cort          94  (43.0, 108.0]\n",
       "196  44a154410273      cort          89  (43.0, 108.0]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cell_type</th>\n",
       "      <th>annotation</th>\n",
       "      <th>quantiles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c4121689002f</td>\n",
       "      <td>shsy5y</td>\n",
       "      <td>790</td>\n",
       "      <td>(447.8, 790.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d164e96bb7a9</td>\n",
       "      <td>shsy5y</td>\n",
       "      <td>782</td>\n",
       "      <td>(447.8, 790.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e748ac1c469b</td>\n",
       "      <td>shsy5y</td>\n",
       "      <td>703</td>\n",
       "      <td>(447.8, 790.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aff8fb4fc364</td>\n",
       "      <td>shsy5y</td>\n",
       "      <td>609</td>\n",
       "      <td>(447.8, 790.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>e8ae919aa92e</td>\n",
       "      <td>shsy5y</td>\n",
       "      <td>605</td>\n",
       "      <td>(447.8, 790.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id cell_type  annotation       quantiles\n",
       "0  c4121689002f    shsy5y         790  (447.8, 790.0]\n",
       "1  d164e96bb7a9    shsy5y         782  (447.8, 790.0]\n",
       "2  e748ac1c469b    shsy5y         703  (447.8, 790.0]\n",
       "3  aff8fb4fc364    shsy5y         609  (447.8, 790.0]\n",
       "4  e8ae919aa92e    shsy5y         605  (447.8, 790.0]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_images = df_base.groupby([\"id\", \"cell_type\"]).agg({'annotation': 'count'}).sort_values(\"annotation\", ascending=False).reset_index()\n",
    "\n",
    "for ct in cell_type_dict:\n",
    "    ctdf = df_images[df_images[\"cell_type\"]==ct].copy()\n",
    "    if len(ctdf)>0:\n",
    "        ctdf['quantiles'] = pd.qcut(ctdf['annotation'], 5,duplicates=what_to_do_with_duplicates)\n",
    "        display(ctdf.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "DQeF1Zi00zRf",
    "outputId": "af89f7a7-245f-4583-85e3-4f4872c62171",
    "papermill": {
     "duration": 0.053443,
     "end_time": "2021-10-30T15:50:08.713717",
     "exception": false,
     "start_time": "2021-10-30T15:50:08.660274",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cell_type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>astro</th>\n",
       "      <td>131</td>\n",
       "      <td>80</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>73</td>\n",
       "      <td>100</td>\n",
       "      <td>594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cort</th>\n",
       "      <td>320</td>\n",
       "      <td>33</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>23</td>\n",
       "      <td>30</td>\n",
       "      <td>39</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shsy5y</th>\n",
       "      <td>155</td>\n",
       "      <td>337</td>\n",
       "      <td>149</td>\n",
       "      <td>49</td>\n",
       "      <td>235</td>\n",
       "      <td>324</td>\n",
       "      <td>429</td>\n",
       "      <td>790</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           count  mean  std  min  25%  50%  75%  max\n",
       "cell_type                                           \n",
       "astro        131    80   64    5   50   73  100  594\n",
       "cort         320    33   16    4   23   30   39  108\n",
       "shsy5y       155   337  149   49  235  324  429  790"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_images.groupby(\"cell_type\").annotation.describe().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "oCRxcK2f0zRf",
    "outputId": "ea5a6673-5758-4964-cdde-03235c5c8215",
    "papermill": {
     "duration": 0.041853,
     "end_time": "2021-10-30T15:50:08.783532",
     "exception": false,
     "start_time": "2021-10-30T15:50:08.741679",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot interpret '<attribute 'dtype' of 'numpy.generic' objects>' as a data type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-8e6ed9999413>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# We used this as a reference to fill BOX_DETECTIONS_PER_IMG=140\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_images\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'annotation'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/python3/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdescribe\u001b[0;34m(self, percentiles, include, exclude)\u001b[0m\n\u001b[1;32m  10263\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minclude\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexclude\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10264\u001b[0m             \u001b[0;31m# when some numerics are found, keep only numerics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m> 10265\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_dtypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m  10266\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10267\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mselect_dtypes\u001b[0;34m(self, include, exclude)\u001b[0m\n\u001b[1;32m   3440\u001b[0m         \u001b[0;31m# the \"union\" of the logic of case 1 and case 2:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3441\u001b[0m         \u001b[0;31m# we get the included and excluded, and return their logical and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3442\u001b[0;31m         \u001b[0minclude_these\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minclude\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3443\u001b[0m         \u001b[0mexclude_these\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    312\u001b[0m                     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msanitize_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_cast_failure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSingleBlockManager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfastpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36msanitize_array\u001b[0;34m(data, index, dtype, copy, raise_cast_failure)\u001b[0m\n\u001b[1;32m    710\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_cast_to_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 712\u001b[0;31m             \u001b[0msubarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstruct_1d_arraylike_from_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/pandas/core/dtypes/cast.py\u001b[0m in \u001b[0;36mconstruct_1d_arraylike_from_scalar\u001b[0;34m(value, length, dtype)\u001b[0m\n\u001b[1;32m   1239\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1241\u001b[0;31m         \u001b[0msubarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1242\u001b[0m         \u001b[0msubarr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot interpret '<attribute 'dtype' of 'numpy.generic' objects>' as a data type"
     ]
    }
   ],
   "source": [
    "# We used this as a reference to fill BOX_DETECTIONS_PER_IMG=140\n",
    "df_images[['annotation']].describe().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "2v7VvtTp0zRf",
    "outputId": "82690b02-dd4b-4c1d-ed5d-78e30bf084a2",
    "papermill": {
     "duration": 0.057122,
     "end_time": "2021-10-30T15:50:08.872848",
     "exception": false,
     "start_time": "2021-10-30T15:50:08.815726",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images in train set:           560\n",
      "Annotations in train set:      67815\n",
      "Images in validation set:      46\n",
      "Annotations in validation set: 5770\n"
     ]
    }
   ],
   "source": [
    "# Use the quantiles of amoount of annotations to stratify\n",
    "df_images_train, df_images_val = train_test_split(df_images, stratify=df_images['cell_type'], \n",
    "                                                  test_size=PCT_IMAGES_VALIDATION,\n",
    "                                                  random_state=1234)\n",
    "df_train = df_base[df_base['id'].isin(df_images_train['id'])]\n",
    "df_val = df_base[df_base['id'].isin(df_images_val['id'])]\n",
    "print(f\"Images in train set:           {len(df_images_train)}\")\n",
    "print(f\"Annotations in train set:      {len(df_train)}\")\n",
    "print(f\"Images in validation set:      {len(df_images_val)}\")\n",
    "print(f\"Annotations in validation set: {len(df_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resize_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "kUcpAbdO0zRg",
    "papermill": {
     "duration": 0.113642,
     "end_time": "2021-10-30T15:50:09.011921",
     "exception": false,
     "start_time": "2021-10-30T15:50:08.898279",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_train = CellDataset(TRAIN_PATH, df_train, resize=resize_factor, transforms=get_transform(train=True))\n",
    "dl_train = DataLoader(ds_train, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True,\n",
    "                      num_workers=2, collate_fn=lambda x: tuple(zip(*x)))\n",
    "\n",
    "ds_val = CellDataset(TRAIN_PATH, df_val, resize=resize_factor, transforms=get_transform(train=False))\n",
    "dl_val = DataLoader(ds_val, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True,\n",
    "                    num_workers=2, collate_fn=lambda x: tuple(zip(*x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y8JNMn770zRg",
    "papermill": {
     "duration": 0.026132,
     "end_time": "2021-10-30T15:50:09.063742",
     "exception": false,
     "start_time": "2021-10-30T15:50:09.03761",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.026483,
     "end_time": "2021-10-30T15:50:09.116837",
     "exception": false,
     "start_time": "2021-10-30T15:50:09.090354",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## setup model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf /home/ubuntu/.cache/torch/hub/checkpoints/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "VMaqdcNa0zRg",
    "outputId": "c5ca31a5-6d8d-4639-e547-f44e5772c725",
    "papermill": {
     "duration": 4.941361,
     "end_time": "2021-10-30T15:50:14.083567",
     "exception": false,
     "start_time": "2021-10-30T15:50:09.142206",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Override pythorch checkpoint with an \"offline\" version of the file\n",
    "!mkdir -p /home/ubuntu/.cache/torch/hub/checkpoints/\n",
    "!cp /home/ubuntu/kaggle-sartorius/data/raw/cocopre/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth /home/ubuntu/.cache/torch/hub/checkpoints/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backbone = torchvision.models.resnet101(pretrained=True)\n",
    "# backbone.out_channels = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = torchvision.models.detection.MaskRCNN(\n",
    "#         backbone,\n",
    "#         num_classes=3,\n",
    "#         min_size=HEIGHT,\n",
    "#         max_size=WIDTH,\n",
    "#         image_mean=RESNET_MEAN,\n",
    "#         image_std=RESNET_STD,\n",
    "#         rpn_anchor_generator=None,\n",
    "#         rpn_head=None,\n",
    "#         rpn_pre_nms_top_n_train=2000,\n",
    "#         rpn_pre_nms_top_n_test=1000,\n",
    "#         rpn_post_nms_top_n_train=2000,\n",
    "#         rpn_post_nms_top_n_test=1000,\n",
    "#         rpn_nms_thresh=0.7,\n",
    "#         rpn_fg_iou_thresh=0.7,\n",
    "#         rpn_bg_iou_thresh=0.3,\n",
    "#         rpn_batch_size_per_image=256,\n",
    "#         rpn_positive_fraction=0.5,\n",
    "#         rpn_score_thresh=0.0,\n",
    "#         box_roi_pool=None,\n",
    "#         box_head=None,\n",
    "#         box_predictor=None,\n",
    "#         box_score_thresh=0.05,\n",
    "#         box_nms_thresh=0.5,\n",
    "#         box_detections_per_img=BOX_DETECTIONS_PER_IMG,\n",
    "#         box_fg_iou_thresh=0.5,\n",
    "#         box_bg_iou_thresh=0.5,\n",
    "#         box_batch_size_per_image=512,\n",
    "#         box_positive_fraction=0.25,\n",
    "#         bbox_reg_weights=None,\n",
    "#         mask_roi_pool=None,\n",
    "#         mask_head=None,\n",
    "#         mask_predictor=None,\n",
    "#     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_instance_segmentation_model_anchors(num_classes):\n",
    "#     #load an instance segmentation model pre-trained on COCO\n",
    "#     model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=False)\n",
    "\n",
    "#     #create an anchor_generator for the FPN which by default has 5 outputs\n",
    "#     anchor_generator = AnchorGenerator(\n",
    "#                 sizes=((16,), (32,), (64,), (128,), (256,)),\n",
    "#                 aspect_ratios=tuple([(0.25, 0.5, 1.0, 2.0) for _ in range(5)]))\n",
    "\n",
    "#     model.rpn.anchor_generator = anchor_generator\n",
    "\n",
    "#     # 256 because that's the number of features that FPN returns\n",
    "#     model.rpn.head = RPNHead(256, anchor_generator.num_anchors_per_location()[0])\n",
    "\n",
    "#     # get the number of input features for the classifier\n",
    "#     in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "#     # replace the pre-trained head with a new one\n",
    "#     model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "#     # now get the number of input features for the mask classifiers\n",
    "#     in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "#     hidden_layer = 256\n",
    "\n",
    "#     # and replace the mask predictor with a new one\n",
    "#     model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, hidden_layer,num_classes)\n",
    "\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(num_classes, model_chkpt=None):\n",
    "    # This is just a dummy value for the classification head\n",
    "    \n",
    "    if NORMALIZE:\n",
    "        model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True,\n",
    "                                                                   box_detections_per_img=BOX_DETECTIONS_PER_IMG,\n",
    "                                                                   image_mean=RESNET_MEAN,\n",
    "                                                                   image_std=RESNET_STD)\n",
    "    else:\n",
    "        model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True,\n",
    "                                                                   trainable_backbone_layers=5,\n",
    "                                                                   box_detections_per_img=BOX_DETECTIONS_PER_IMG)\n",
    "\n",
    "    \n",
    "    # get the number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    \n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes+1)\n",
    "\n",
    "    # now get the number of input features for the mask classifier\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 512\n",
    "\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, hidden_layer, num_classes+1)\n",
    "    \n",
    "    if model_chkpt:\n",
    "        model.load_state_dict(torch.load(model_chkpt, map_location=DEVICE))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Mask R-CNN model\n",
    "# The model does classification, bounding boxes and MASKs for individuals, all at the same time\n",
    "model = get_model(len(cell_type_dict))\n",
    "#model = get_instance_segmentation_model_anchors(len(cell_type_dict))\n",
    "model.to(DEVICE)\n",
    "\n",
    "# TODO: try removing this for\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "    \n",
    "model.train();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RvawgUM30zRh",
    "papermill": {
     "duration": 0.028196,
     "end_time": "2021-10-30T15:50:17.847237",
     "exception": false,
     "start_time": "2021-10-30T15:50:17.819041",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Training loop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.003\n",
    "USE_SCHEDULER = True\n",
    "STEP_SIZE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cp /home/ubuntu/kaggle-sartorius/notebooks/models/MASK-RCNN/pytorch_model-e17.bin /home/ubuntu/.cache/torch/hub/checkpoints/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "52B16JCW0zRh",
    "outputId": "9b9c5ad9-58c1-4d50-dd7c-79b18b1e57b7",
    "papermill": {
     "duration": 7670.472684,
     "end_time": "2021-10-30T17:58:08.347309",
     "exception": false,
     "start_time": "2021-10-30T15:50:17.874625",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1 of 100\n",
      "[Epoch  1 / 100]\n",
      "[Epoch  1 / 100] Train mask-only loss:   0.433, classifier loss   0.563\n",
      "[Epoch  1 / 100] Val mask-only loss  :   0.326, classifier loss   0.399\n",
      "[Epoch  1 / 100]\n",
      "[Epoch  1 / 100] Train loss:   2.083. Val loss:   1.647 [129 secs]\n",
      "[Epoch  1 / 100]\n",
      "Starting epoch 2 of 100\n",
      "[Epoch  2 / 100]\n",
      "[Epoch  2 / 100] Train mask-only loss:   0.327, classifier loss   0.391\n",
      "[Epoch  2 / 100] Val mask-only loss  :   0.316, classifier loss   0.357\n",
      "[Epoch  2 / 100]\n",
      "[Epoch  2 / 100] Train loss:   1.616. Val loss:   1.528 [124 secs]\n",
      "[Epoch  2 / 100]\n",
      "Starting epoch 3 of 100\n",
      "[Epoch  3 / 100]\n",
      "[Epoch  3 / 100] Train mask-only loss:   0.318, classifier loss   0.355\n",
      "[Epoch  3 / 100] Val mask-only loss  :   0.314, classifier loss   0.330\n",
      "[Epoch  3 / 100]\n",
      "[Epoch  3 / 100] Train loss:   1.519. Val loss:   1.481 [134 secs]\n",
      "[Epoch  3 / 100]\n",
      "Starting epoch 4 of 100\n",
      "[Epoch  4 / 100]\n",
      "[Epoch  4 / 100] Train mask-only loss:   0.315, classifier loss   0.337\n",
      "[Epoch  4 / 100] Val mask-only loss  :   0.310, classifier loss   0.336\n",
      "[Epoch  4 / 100]\n",
      "[Epoch  4 / 100] Train loss:   1.479. Val loss:   1.503 [132 secs]\n",
      "[Epoch  4 / 100]\n",
      "Starting epoch 5 of 100\n",
      "[Epoch  5 / 100]\n",
      "[Epoch  5 / 100] Train mask-only loss:   0.311, classifier loss   0.329\n",
      "[Epoch  5 / 100] Val mask-only loss  :   0.309, classifier loss   0.350\n",
      "[Epoch  5 / 100]\n",
      "[Epoch  5 / 100] Train loss:   1.454. Val loss:   1.511 [138 secs]\n",
      "[Epoch  5 / 100]\n",
      "Starting epoch 6 of 100\n",
      "[Epoch  6 / 100]\n",
      "[Epoch  6 / 100] Train mask-only loss:   0.311, classifier loss   0.329\n",
      "[Epoch  6 / 100] Val mask-only loss  :   0.301, classifier loss   0.310\n",
      "[Epoch  6 / 100]\n",
      "[Epoch  6 / 100] Train loss:   1.449. Val loss:   1.381 [136 secs]\n",
      "[Epoch  6 / 100]\n",
      "Starting epoch 7 of 100\n",
      "[Epoch  7 / 100]\n",
      "[Epoch  7 / 100] Train mask-only loss:   0.308, classifier loss   0.315\n",
      "[Epoch  7 / 100] Val mask-only loss  :   0.304, classifier loss   0.312\n",
      "[Epoch  7 / 100]\n",
      "[Epoch  7 / 100] Train loss:   1.417. Val loss:   1.439 [139 secs]\n",
      "[Epoch  7 / 100]\n",
      "Starting epoch 8 of 100\n",
      "[Epoch  8 / 100]\n",
      "[Epoch  8 / 100] Train mask-only loss:   0.307, classifier loss   0.305\n",
      "[Epoch  8 / 100] Val mask-only loss  :   0.309, classifier loss   0.308\n",
      "[Epoch  8 / 100]\n",
      "[Epoch  8 / 100] Train loss:   1.398. Val loss:   1.452 [136 secs]\n",
      "[Epoch  8 / 100]\n",
      "Starting epoch 9 of 100\n",
      "[Epoch  9 / 100]\n",
      "[Epoch  9 / 100] Train mask-only loss:   0.306, classifier loss   0.302\n",
      "[Epoch  9 / 100] Val mask-only loss  :   0.300, classifier loss   0.300\n",
      "[Epoch  9 / 100]\n",
      "[Epoch  9 / 100] Train loss:   1.388. Val loss:   1.411 [137 secs]\n",
      "[Epoch  9 / 100]\n",
      "Starting epoch 10 of 100\n",
      "[Epoch 10 / 100]\n",
      "[Epoch 10 / 100] Train mask-only loss:   0.305, classifier loss   0.299\n",
      "[Epoch 10 / 100] Val mask-only loss  :   0.305, classifier loss   0.295\n",
      "[Epoch 10 / 100]\n",
      "[Epoch 10 / 100] Train loss:   1.375. Val loss:   1.394 [135 secs]\n",
      "[Epoch 10 / 100]\n",
      "Starting epoch 11 of 100\n",
      "[Epoch 11 / 100]\n",
      "[Epoch 11 / 100] Train mask-only loss:   0.301, classifier loss   0.283\n",
      "[Epoch 11 / 100] Val mask-only loss  :   0.299, classifier loss   0.297\n",
      "[Epoch 11 / 100]\n",
      "[Epoch 11 / 100] Train loss:   1.332. Val loss:   1.383 [134 secs]\n",
      "[Epoch 11 / 100]\n",
      "Starting epoch 12 of 100\n",
      "[Epoch 12 / 100]\n",
      "[Epoch 12 / 100] Train mask-only loss:   0.301, classifier loss   0.281\n",
      "[Epoch 12 / 100] Val mask-only loss  :   0.298, classifier loss   0.289\n",
      "[Epoch 12 / 100]\n",
      "[Epoch 12 / 100] Train loss:   1.324. Val loss:   1.367 [144 secs]\n",
      "[Epoch 12 / 100]\n",
      "Starting epoch 13 of 100\n",
      "[Epoch 13 / 100]\n",
      "[Epoch 13 / 100] Train mask-only loss:   0.302, classifier loss   0.279\n",
      "[Epoch 13 / 100] Val mask-only loss  :   0.297, classifier loss   0.302\n",
      "[Epoch 13 / 100]\n",
      "[Epoch 13 / 100] Train loss:   1.323. Val loss:   1.371 [135 secs]\n",
      "[Epoch 13 / 100]\n",
      "Starting epoch 14 of 100\n",
      "[Epoch 14 / 100]\n",
      "[Epoch 14 / 100] Train mask-only loss:   0.302, classifier loss   0.278\n",
      "[Epoch 14 / 100] Val mask-only loss  :   0.298, classifier loss   0.292\n",
      "[Epoch 14 / 100]\n",
      "[Epoch 14 / 100] Train loss:   1.320. Val loss:   1.369 [139 secs]\n",
      "[Epoch 14 / 100]\n",
      "Starting epoch 15 of 100\n",
      "[Epoch 15 / 100]\n",
      "[Epoch 15 / 100] Train mask-only loss:   0.301, classifier loss   0.279\n",
      "[Epoch 15 / 100] Val mask-only loss  :   0.297, classifier loss   0.290\n",
      "[Epoch 15 / 100]\n",
      "[Epoch 15 / 100] Train loss:   1.322. Val loss:   1.366 [137 secs]\n",
      "[Epoch 15 / 100]\n",
      "Starting epoch 16 of 100\n",
      "[Epoch 16 / 100]\n",
      "[Epoch 16 / 100] Train mask-only loss:   0.302, classifier loss   0.276\n",
      "[Epoch 16 / 100] Val mask-only loss  :   0.294, classifier loss   0.294\n",
      "[Epoch 16 / 100]\n",
      "[Epoch 16 / 100] Train loss:   1.314. Val loss:   1.365 [137 secs]\n",
      "[Epoch 16 / 100]\n",
      "Starting epoch 17 of 100\n",
      "[Epoch 17 / 100]\n",
      "[Epoch 17 / 100] Train mask-only loss:   0.301, classifier loss   0.277\n",
      "[Epoch 17 / 100] Val mask-only loss  :   0.299, classifier loss   0.297\n",
      "[Epoch 17 / 100]\n",
      "[Epoch 17 / 100] Train loss:   1.314. Val loss:   1.386 [140 secs]\n",
      "[Epoch 17 / 100]\n",
      "Starting epoch 18 of 100\n",
      "[Epoch 18 / 100]\n",
      "[Epoch 18 / 100] Train mask-only loss:   0.301, classifier loss   0.277\n",
      "[Epoch 18 / 100] Val mask-only loss  :   0.301, classifier loss   0.289\n",
      "[Epoch 18 / 100]\n",
      "[Epoch 18 / 100] Train loss:   1.318. Val loss:   1.365 [137 secs]\n",
      "[Epoch 18 / 100]\n",
      "Starting epoch 19 of 100\n",
      "[Epoch 19 / 100]\n",
      "[Epoch 19 / 100] Train mask-only loss:   0.302, classifier loss   0.277\n",
      "[Epoch 19 / 100] Val mask-only loss  :   0.299, classifier loss   0.288\n",
      "[Epoch 19 / 100]\n",
      "[Epoch 19 / 100] Train loss:   1.312. Val loss:   1.365 [139 secs]\n",
      "[Epoch 19 / 100]\n",
      "Starting epoch 20 of 100\n",
      "[Epoch 20 / 100]\n",
      "[Epoch 20 / 100] Train mask-only loss:   0.301, classifier loss   0.275\n",
      "[Epoch 20 / 100] Val mask-only loss  :   0.298, classifier loss   0.284\n",
      "[Epoch 20 / 100]\n",
      "[Epoch 20 / 100] Train loss:   1.309. Val loss:   1.366 [127 secs]\n",
      "[Epoch 20 / 100]\n",
      "Starting epoch 21 of 100\n",
      "[Epoch 21 / 100]\n",
      "[Epoch 21 / 100] Train mask-only loss:   0.301, classifier loss   0.272\n",
      "[Epoch 21 / 100] Val mask-only loss  :   0.303, classifier loss   0.284\n",
      "[Epoch 21 / 100]\n",
      "[Epoch 21 / 100] Train loss:   1.306. Val loss:   1.369 [136 secs]\n",
      "[Epoch 21 / 100]\n",
      "Starting epoch 22 of 100\n",
      "[Epoch 22 / 100]\n",
      "[Epoch 22 / 100] Train mask-only loss:   0.301, classifier loss   0.273\n",
      "[Epoch 22 / 100] Val mask-only loss  :   0.298, classifier loss   0.279\n",
      "[Epoch 22 / 100]\n",
      "[Epoch 22 / 100] Train loss:   1.302. Val loss:   1.343 [121 secs]\n",
      "[Epoch 22 / 100]\n",
      "Starting epoch 23 of 100\n",
      "[Epoch 23 / 100]\n",
      "[Epoch 23 / 100] Train mask-only loss:   0.301, classifier loss   0.272\n",
      "[Epoch 23 / 100] Val mask-only loss  :   0.297, classifier loss   0.281\n",
      "[Epoch 23 / 100]\n",
      "[Epoch 23 / 100] Train loss:   1.305. Val loss:   1.344 [141 secs]\n",
      "[Epoch 23 / 100]\n",
      "Starting epoch 24 of 100\n",
      "[Epoch 24 / 100]\n",
      "[Epoch 24 / 100] Train mask-only loss:   0.300, classifier loss   0.274\n",
      "[Epoch 24 / 100] Val mask-only loss  :   0.297, classifier loss   0.289\n",
      "[Epoch 24 / 100]\n",
      "[Epoch 24 / 100] Train loss:   1.305. Val loss:   1.381 [132 secs]\n",
      "[Epoch 24 / 100]\n",
      "Starting epoch 25 of 100\n",
      "[Epoch 25 / 100]\n",
      "[Epoch 25 / 100] Train mask-only loss:   0.300, classifier loss   0.272\n",
      "[Epoch 25 / 100] Val mask-only loss  :   0.299, classifier loss   0.293\n",
      "[Epoch 25 / 100]\n",
      "[Epoch 25 / 100] Train loss:   1.303. Val loss:   1.362 [134 secs]\n",
      "[Epoch 25 / 100]\n",
      "Starting epoch 26 of 100\n",
      "[Epoch 26 / 100]\n",
      "[Epoch 26 / 100] Train mask-only loss:   0.300, classifier loss   0.271\n",
      "[Epoch 26 / 100] Val mask-only loss  :   0.299, classifier loss   0.284\n",
      "[Epoch 26 / 100]\n",
      "[Epoch 26 / 100] Train loss:   1.302. Val loss:   1.357 [142 secs]\n",
      "[Epoch 26 / 100]\n",
      "Starting epoch 27 of 100\n",
      "[Epoch 27 / 100]\n",
      "[Epoch 27 / 100] Train mask-only loss:   0.301, classifier loss   0.271\n",
      "[Epoch 27 / 100] Val mask-only loss  :   0.295, classifier loss   0.278\n",
      "[Epoch 27 / 100]\n",
      "[Epoch 27 / 100] Train loss:   1.300. Val loss:   1.333 [127 secs]\n",
      "[Epoch 27 / 100]\n",
      "Starting epoch 28 of 100\n",
      "[Epoch 28 / 100]\n",
      "[Epoch 28 / 100] Train mask-only loss:   0.301, classifier loss   0.274\n",
      "[Epoch 28 / 100] Val mask-only loss  :   0.299, classifier loss   0.287\n",
      "[Epoch 28 / 100]\n",
      "[Epoch 28 / 100] Train loss:   1.306. Val loss:   1.357 [128 secs]\n",
      "[Epoch 28 / 100]\n",
      "Starting epoch 29 of 100\n",
      "[Epoch 29 / 100]\n",
      "[Epoch 29 / 100] Train mask-only loss:   0.301, classifier loss   0.274\n",
      "[Epoch 29 / 100] Val mask-only loss  :   0.297, classifier loss   0.285\n",
      "[Epoch 29 / 100]\n",
      "[Epoch 29 / 100] Train loss:   1.308. Val loss:   1.345 [126 secs]\n",
      "[Epoch 29 / 100]\n",
      "Starting epoch 30 of 100\n",
      "[Epoch 30 / 100]\n",
      "[Epoch 30 / 100] Train mask-only loss:   0.300, classifier loss   0.272\n",
      "[Epoch 30 / 100] Val mask-only loss  :   0.299, classifier loss   0.279\n",
      "[Epoch 30 / 100]\n",
      "[Epoch 30 / 100] Train loss:   1.302. Val loss:   1.337 [130 secs]\n",
      "[Epoch 30 / 100]\n",
      "Starting epoch 31 of 100\n",
      "[Epoch 31 / 100]\n",
      "[Epoch 31 / 100] Train mask-only loss:   0.300, classifier loss   0.272\n",
      "[Epoch 31 / 100] Val mask-only loss  :   0.296, classifier loss   0.274\n",
      "[Epoch 31 / 100]\n",
      "[Epoch 31 / 100] Train loss:   1.304. Val loss:   1.335 [131 secs]\n",
      "[Epoch 31 / 100]\n",
      "Starting epoch 32 of 100\n",
      "[Epoch 32 / 100]\n",
      "[Epoch 32 / 100] Train mask-only loss:   0.301, classifier loss   0.271\n",
      "[Epoch 32 / 100] Val mask-only loss  :   0.301, classifier loss   0.297\n",
      "[Epoch 32 / 100]\n",
      "[Epoch 32 / 100] Train loss:   1.303. Val loss:   1.373 [135 secs]\n",
      "[Epoch 32 / 100]\n",
      "Starting epoch 33 of 100\n",
      "[Epoch 33 / 100]\n",
      "[Epoch 33 / 100] Train mask-only loss:   0.301, classifier loss   0.274\n",
      "[Epoch 33 / 100] Val mask-only loss  :   0.300, classifier loss   0.286\n",
      "[Epoch 33 / 100]\n",
      "[Epoch 33 / 100] Train loss:   1.307. Val loss:   1.372 [132 secs]\n",
      "[Epoch 33 / 100]\n",
      "Starting epoch 34 of 100\n",
      "[Epoch 34 / 100]\n",
      "[Epoch 34 / 100] Train mask-only loss:   0.300, classifier loss   0.273\n",
      "[Epoch 34 / 100] Val mask-only loss  :   0.300, classifier loss   0.283\n",
      "[Epoch 34 / 100]\n",
      "[Epoch 34 / 100] Train loss:   1.303. Val loss:   1.352 [129 secs]\n",
      "[Epoch 34 / 100]\n",
      "Starting epoch 35 of 100\n",
      "[Epoch 35 / 100]\n",
      "[Epoch 35 / 100] Train mask-only loss:   0.300, classifier loss   0.273\n",
      "[Epoch 35 / 100] Val mask-only loss  :   0.298, classifier loss   0.283\n",
      "[Epoch 35 / 100]\n",
      "[Epoch 35 / 100] Train loss:   1.304. Val loss:   1.347 [132 secs]\n",
      "[Epoch 35 / 100]\n",
      "Starting epoch 36 of 100\n",
      "[Epoch 36 / 100]\n",
      "[Epoch 36 / 100] Train mask-only loss:   0.300, classifier loss   0.273\n",
      "[Epoch 36 / 100] Val mask-only loss  :   0.298, classifier loss   0.281\n",
      "[Epoch 36 / 100]\n",
      "[Epoch 36 / 100] Train loss:   1.304. Val loss:   1.357 [133 secs]\n",
      "[Epoch 36 / 100]\n",
      "Starting epoch 37 of 100\n",
      "[Epoch 37 / 100]\n",
      "[Epoch 37 / 100] Train mask-only loss:   0.300, classifier loss   0.272\n",
      "[Epoch 37 / 100] Val mask-only loss  :   0.299, classifier loss   0.290\n",
      "[Epoch 37 / 100]\n",
      "[Epoch 37 / 100] Train loss:   1.301. Val loss:   1.354 [135 secs]\n",
      "[Epoch 37 / 100]\n",
      "Starting epoch 38 of 100\n",
      "[Epoch 38 / 100]\n",
      "[Epoch 38 / 100] Train mask-only loss:   0.301, classifier loss   0.271\n",
      "[Epoch 38 / 100] Val mask-only loss  :   0.300, classifier loss   0.289\n",
      "[Epoch 38 / 100]\n",
      "[Epoch 38 / 100] Train loss:   1.304. Val loss:   1.353 [134 secs]\n",
      "[Epoch 38 / 100]\n",
      "Starting epoch 39 of 100\n",
      "[Epoch 39 / 100]\n",
      "[Epoch 39 / 100] Train mask-only loss:   0.300, classifier loss   0.272\n",
      "[Epoch 39 / 100] Val mask-only loss  :   0.301, classifier loss   0.296\n",
      "[Epoch 39 / 100]\n",
      "[Epoch 39 / 100] Train loss:   1.301. Val loss:   1.383 [135 secs]\n",
      "[Epoch 39 / 100]\n",
      "Starting epoch 40 of 100\n",
      "[Epoch 40 / 100]\n",
      "[Epoch 40 / 100] Train mask-only loss:   0.300, classifier loss   0.272\n",
      "[Epoch 40 / 100] Val mask-only loss  :   0.300, classifier loss   0.285\n",
      "[Epoch 40 / 100]\n",
      "[Epoch 40 / 100] Train loss:   1.301. Val loss:   1.357 [129 secs]\n",
      "[Epoch 40 / 100]\n",
      "Starting epoch 41 of 100\n",
      "[Epoch 41 / 100]\n",
      "[Epoch 41 / 100] Train mask-only loss:   0.300, classifier loss   0.272\n",
      "[Epoch 41 / 100] Val mask-only loss  :   0.300, classifier loss   0.285\n",
      "[Epoch 41 / 100]\n",
      "[Epoch 41 / 100] Train loss:   1.304. Val loss:   1.365 [130 secs]\n",
      "[Epoch 41 / 100]\n",
      "Starting epoch 42 of 100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-8bcda4d8ce1a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mloss_mask_accum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mloss_classifier_accum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# Predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1186\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1187\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1140\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1142\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1143\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    177\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-86:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3/dist-packages/torch/utils/data/_utils/pin_memory.py\", line 28, in _pin_memory_loop\n",
      "    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 116, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/usr/lib/python3/dist-packages/torch/multiprocessing/reductions.py\", line 289, in rebuild_storage_fd\n",
      "    fd = df.detach()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/resource_sharer.py\", line 57, in detach\n",
      "    with _resource_sharer.get_connection(self._id) as conn:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/resource_sharer.py\", line 87, in get_connection\n",
      "    c = Client(address, authkey=process.current_process().authkey)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 502, in Client\n",
      "    c = SocketClient(address)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 630, in SocketClient\n",
      "    s.connect(address)\n",
      "FileNotFoundError: [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=LEARNING_RATE, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
    "#optimizer = torch.optim.Adam(params, lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=0.1)\n",
    "\n",
    "n_batches, n_batches_val = len(dl_train), len(dl_val)\n",
    "\n",
    "validation_mask_losses = []\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    print(f\"Starting epoch {epoch} of {NUM_EPOCHS}\")\n",
    "\n",
    "    time_start = time.time()\n",
    "    loss_accum = 0.0\n",
    "    loss_mask_accum = 0.0\n",
    "    loss_classifier_accum = 0.0\n",
    "    for batch_idx, (images, targets) in enumerate(dl_train, 1):\n",
    "    \n",
    "        # Predict\n",
    "        images = list(image.to(DEVICE) for image in images)\n",
    "        targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "        loss = sum(loss for loss in loss_dict.values())\n",
    "        \n",
    "        # Backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Logging\n",
    "        loss_mask = loss_dict['loss_mask'].item()\n",
    "        loss_accum += loss.item()\n",
    "        loss_mask_accum += loss_mask\n",
    "        loss_classifier_accum += loss_dict['loss_classifier'].item()\n",
    "        \n",
    "        if batch_idx % 500 == 0:\n",
    "            print(f\"    [Batch {batch_idx:3d} / {n_batches:3d}] Batch train loss: {loss.item():7.3f}. Mask-only loss: {loss_mask:7.3f}.\")\n",
    "                        \n",
    "    if USE_SCHEDULER:\n",
    "        lr_scheduler.step()\n",
    "\n",
    "    # Train losses\n",
    "    train_loss = loss_accum / n_batches\n",
    "    train_loss_mask = loss_mask_accum / n_batches\n",
    "    train_loss_classifier = loss_classifier_accum / n_batches\n",
    "\n",
    "    # Validation\n",
    "    val_loss_accum = 0\n",
    "    val_loss_mask_accum = 0\n",
    "    val_loss_classifier_accum = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, targets) in enumerate(dl_val, 1):\n",
    "            images = list(image.to(DEVICE) for image in images)\n",
    "            targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            val_loss_dict = model(images, targets)\n",
    "            val_batch_loss = sum(loss for loss in val_loss_dict.values())\n",
    "            val_loss_accum += val_batch_loss.item()\n",
    "            val_loss_mask_accum += val_loss_dict['loss_mask'].item()\n",
    "            val_loss_classifier_accum += val_loss_dict['loss_classifier'].item()\n",
    "\n",
    "    # Validation losses\n",
    "    val_loss = val_loss_accum / n_batches_val\n",
    "    val_loss_mask = val_loss_mask_accum / n_batches_val\n",
    "    val_loss_classifier = val_loss_classifier_accum / n_batches_val\n",
    "    elapsed = time.time() - time_start\n",
    "\n",
    "    validation_mask_losses.append(val_loss_mask)\n",
    "\n",
    "    torch.save(model.state_dict(), f\"models/MASK-RCNN/pytorch_model-e{epoch}.bin\")\n",
    "    prefix = f\"[Epoch {epoch:2d} / {NUM_EPOCHS:2d}]\"\n",
    "    print(prefix)\n",
    "    print(f\"{prefix} Train mask-only loss: {train_loss_mask:7.3f}, classifier loss {train_loss_classifier:7.3f}\")\n",
    "    print(f\"{prefix} Val mask-only loss  : {val_loss_mask:7.3f}, classifier loss {val_loss_classifier:7.3f}\")\n",
    "    print(prefix)\n",
    "    print(f\"{prefix} Train loss: {train_loss:7.3f}. Val loss: {val_loss:7.3f} [{elapsed:.0f} secs]\")\n",
    "    print(prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MspyyJlP0zRh",
    "papermill": {
     "duration": 0.054203,
     "end_time": "2021-10-30T17:58:08.456349",
     "exception": false,
     "start_time": "2021-10-30T17:58:08.402146",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Analyze prediction results for train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dJOE3B0u0zRi",
    "papermill": {
     "duration": 2.317703,
     "end_time": "2021-10-30T17:58:10.828377",
     "exception": false,
     "start_time": "2021-10-30T17:58:08.510674",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plots: the image, The image + the ground truth mask, The image + the predicted mask\n",
    "\n",
    "def analyze_train_sample(model, ds_train, sample_index):\n",
    "    \n",
    "    img, targets = ds_train[sample_index]\n",
    "    #print(img.shape)\n",
    "    l = np.unique(targets[\"labels\"])\n",
    "    ig, ax = plt.subplots(nrows=1, ncols=3, figsize=(20,60), facecolor=\"#fefefe\")\n",
    "    ax[0].imshow(img.numpy().transpose((1,2,0)))\n",
    "    ax[0].set_title(f\"cell type {l}\")\n",
    "    ax[0].axis(\"off\")\n",
    "    \n",
    "    masks = combine_masks(targets['masks'], 0.5)\n",
    "    #plt.imshow(img.numpy().transpose((1,2,0)))\n",
    "    ax[1].imshow(masks)\n",
    "    ax[1].set_title(f\"Ground truth, {len(targets['masks'])} cells\")\n",
    "    ax[1].axis(\"off\")\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        preds = model([img.to(DEVICE)])[0]\n",
    "    \n",
    "    l = pd.Series(preds['labels'].cpu().numpy()).value_counts()\n",
    "    lstr = \"\"\n",
    "    for i in l.index:\n",
    "        lstr += f\"{l[i]}x{i} \"\n",
    "    #print(l, l.sort_values().index[-1])\n",
    "    #plt.imshow(img.cpu().numpy().transpose((1,2,0)))\n",
    "    mask_threshold = mask_threshold_dict[l.sort_values().index[-1]]\n",
    "    #print(mask_threshold)\n",
    "    pred_masks = combine_masks(get_filtered_masks(preds), mask_threshold)\n",
    "    ax[2].imshow(pred_masks)\n",
    "    ax[2].set_title(f\"Predictions, labels: {lstr}\")\n",
    "    ax[2].axis(\"off\")\n",
    "    plt.show() \n",
    "    \n",
    "    #print(masks.shape, pred_masks.shape)\n",
    "    score = iou_map([masks],[pred_masks])\n",
    "    print(\"Score:\", score)    \n",
    "    \n",
    "    \n",
    "# NOTE: It puts the model in eval mode!! Revert for re-training\n",
    "analyze_train_sample(model, ds_train, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZRx9K5n60zRi",
    "outputId": "3ff2b8d0-8e4d-46a0-b3cb-7f2fad862808",
    "papermill": {
     "duration": 1.189143,
     "end_time": "2021-10-30T17:58:12.06041",
     "exception": false,
     "start_time": "2021-10-30T17:58:10.871267",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "analyze_train_sample(model, ds_train, 102)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rn6YeGVZ0zRi",
    "outputId": "3b0b0c2b-1f0b-40b0-9529-275d24c3afa3",
    "papermill": {
     "duration": 5.359614,
     "end_time": "2021-10-30T17:58:17.470546",
     "exception": false,
     "start_time": "2021-10-30T17:58:12.110932",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "analyze_train_sample(model, ds_train, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jon4MSmk0zRj",
    "papermill": {
     "duration": 0.059699,
     "end_time": "2021-10-30T17:58:17.590822",
     "exception": false,
     "start_time": "2021-10-30T17:58:17.531123",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Get the model from the best epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O0ejRcer0zRj",
    "outputId": "0806ad69-abcf-440a-c9ad-d5a2a454abe4",
    "papermill": {
     "duration": 2288.212204,
     "end_time": "2021-10-30T18:36:25.86261",
     "exception": false,
     "start_time": "2021-10-30T17:58:17.650406",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Epochs with their losses and IOU scores\n",
    "\n",
    "val_scores = pd.DataFrame()\n",
    "for e, val_loss in enumerate(validation_mask_losses[:2]):\n",
    "    model_chk = f\"models/MASK-RCNN/pytorch_model-e{e+1}.bin\" \n",
    "    print(\"Loading:\", model_chk)\n",
    "    model = get_model(len(cell_type_dict), model_chk)\n",
    "    model.load_state_dict(torch.load(model_chk))\n",
    "    model = model.to(DEVICE)\n",
    "    val_scores.loc[e,\"mask_loss\"] = val_loss\n",
    "    val_scores.loc[e,\"score\"] = get_score(ds_val, model)\n",
    "    \n",
    "    \n",
    "display(val_scores.sort_values(\"score\", ascending=False))\n",
    "\n",
    "best_epoch = np.argmax(val_scores[\"score\"])\n",
    "print(best_epoch+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bTNGfMuQ0zRi",
    "papermill": {
     "duration": 0.080258,
     "end_time": "2021-10-30T18:36:26.026589",
     "exception": false,
     "start_time": "2021-10-30T18:36:25.946331",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tRSo-FPt0zRi",
    "papermill": {
     "duration": 0.081736,
     "end_time": "2021-10-30T18:36:26.192122",
     "exception": false,
     "start_time": "2021-10-30T18:36:26.110386",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Test Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ijZzdcHB0zRj",
    "papermill": {
     "duration": 0.092733,
     "end_time": "2021-10-30T18:36:26.365881",
     "exception": false,
     "start_time": "2021-10-30T18:36:26.273148",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CellTestDataset(Dataset):\n",
    "    def __init__(self, image_dir, transforms=None, resize=False):\n",
    "        self.transforms = transforms\n",
    "        self.image_dir = image_dir\n",
    "        self.image_ids = [f[:-4]for f in os.listdir(self.image_dir)]\n",
    "        self.should_resize = resize is not False\n",
    "        if self.should_resize:\n",
    "            self.height = int(HEIGHT * resize)\n",
    "            self.width = int(WIDTH * resize)\n",
    "            print(\"image size used:\", self.height, self.width)\n",
    "            \n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.image_ids[idx]\n",
    "        image_path = os.path.join(self.image_dir, image_id + '.png')\n",
    "        image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "        if self.should_resize:\n",
    "            image = cv2.resize(image, (self.width, self.height))\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            image, _ = self.transforms(image=image, target=None)\n",
    "        return {'image': image, 'image_id': image_id}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WbciaVrJ0zRj",
    "outputId": "3ec69aa1-4136-41a1-b853-67d38654ef9a",
    "papermill": {
     "duration": 0.090469,
     "end_time": "2021-10-30T18:36:26.536396",
     "exception": false,
     "start_time": "2021-10-30T18:36:26.445927",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_test = CellTestDataset(TEST_PATH, transforms=get_transform(train=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dUrfObRF0zRk",
    "outputId": "5f0432a3-8b87-4540-f49d-9d033b61bdbc",
    "papermill": {
     "duration": 5.323233,
     "end_time": "2021-10-30T18:36:31.939163",
     "exception": false,
     "start_time": "2021-10-30T18:36:26.61593",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_chk = f\"models/MASK-RCNN/pytorch_model-e{best_epoch+1}.bin\"#f\"pytorch_model-e{best_epoch+1}.bin\"\n",
    "print(\"Loading:\", model_chk)\n",
    "model = get_model(len(cell_type_dict))\n",
    "model.load_state_dict(torch.load(model_chk))\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model.eval();\n",
    "\n",
    "submission = []\n",
    "for sample in ds_test:\n",
    "    img = sample['image']\n",
    "    image_id = sample['image_id']\n",
    "    with torch.no_grad():\n",
    "        result = model([img.to(DEVICE)])[0]\n",
    "    \n",
    "    previous_masks = []\n",
    "    for i, mask in enumerate(result[\"masks\"]):\n",
    "\n",
    "        # Filter-out low-scoring results.\n",
    "        score = result[\"scores\"][i].cpu().item()\n",
    "        label = result[\"labels\"][i].cpu().item()\n",
    "        if score > min_score_dict[label]:\n",
    "            mask = mask.cpu().numpy()\n",
    "            # Keep only highly likely pixels\n",
    "            binary_mask = mask > mask_threshold_dict[label]\n",
    "            binary_mask = remove_overlapping_pixels(binary_mask, previous_masks)\n",
    "            previous_masks.append(binary_mask)\n",
    "            rle = rle_encoding(binary_mask)\n",
    "            submission.append((image_id, rle))\n",
    "\n",
    "    # Add empty prediction if no RLE was generated for this image\n",
    "    all_images_ids = [image_id for image_id, rle in submission]\n",
    "    if image_id not in all_images_ids:\n",
    "        submission.append((image_id, \"\"))\n",
    "\n",
    "df_sub = pd.DataFrame(submission, columns=['id', 'predicted'])\n",
    "df_sub.to_csv(\"submission.csv\", index=False)\n",
    "df_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w73Jvhk20zRk",
    "papermill": {
     "duration": 0.080047,
     "end_time": "2021-10-30T18:36:32.10092",
     "exception": false,
     "start_time": "2021-10-30T18:36:32.020873",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
